{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from posenet.model import PoseNetDinoImproved\n",
    "from nflownet.model import NFlowNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from dataset.tartanair import TartanAirDataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_posenet(posenet_path):\n",
    "    # Initialize FlowNet model\n",
    "    posenet = PoseNetDinoImproved().to(device)\n",
    "    \n",
    "    # Load weights from .pth file\n",
    "    checkpoint = torch.load(posenet_path, map_location=device)\n",
    "    \n",
    "    # Handle different save formats\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    elif 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Load state dict into model\n",
    "    posenet.load_state_dict(state_dict)\n",
    "    posenet.train()  # Set to evaluation mode\n",
    "    \n",
    "    return posenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nflownet(nflownet_path):\n",
    "    # Initialize FlowNet model\n",
    "    nflownet = NFlowNet().to(device)\n",
    "    \n",
    "    # Load weights from .pth file\n",
    "    checkpoint = torch.load(nflownet_path, map_location=device)\n",
    "    \n",
    "    # Handle different save formats\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    elif 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Load state dict into model\n",
    "    nflownet.load_state_dict(state_dict)\n",
    "    nflownet.eval()  # Set to evaluation mode\n",
    "    \n",
    "    return nflownet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\emircan/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\emircan/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\emircan/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\emircan/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "C:\\Users\\emircan\\AppData\\Local\\Temp\\ipykernel_19116\\4256205414.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(posenet_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "posenet_path = r\"D:\\KOC UNIVERSITY\\COMP447\\trainedmodels\\posenet\\posenet2.pth\"\n",
    "nflownet_path = r\"D:\\KOC UNIVERSITY\\COMP447\\trainedmodels\\nflownet\\nflownet_final.pth\"\n",
    "\n",
    "posenet = load_posenet(posenet_path)\n",
    "nflownet = load_nflownet(nflownet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from nflownet.utils import compute_image_gradients\n",
    "import kornia\n",
    "from torchvision import transforms\n",
    "\n",
    "class CheiralityLayer(nn.Module):\n",
    "    def __init__(self, posenet, nflownet):\n",
    "        super().__init__()\n",
    "        self.posenet = posenet\n",
    "        self.nflownet = nflownet\n",
    "        self.nflownet.eval()  # freeze NFlowNet\n",
    "        for p in self.nflownet.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "\n",
    "    def construct_A_B(self, H, W, device):\n",
    "        y, x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "\n",
    "        # A and B: [H, W, 2, 3]\n",
    "        A = torch.stack([\n",
    "            -torch.ones_like(x), torch.zeros_like(x), x,\n",
    "            torch.zeros_like(y), -torch.ones_like(y), y\n",
    "        ], dim=-1).reshape(H, W, 2, 3)\n",
    "\n",
    "        B = torch.stack([\n",
    "            x * y, -(x ** 2 + 1), y,\n",
    "            y ** 2 + 1, -x * y, -x\n",
    "        ], dim=-1).reshape(H, W, 2, 3)\n",
    "\n",
    "        # Reshape to [H*W, 2, 3]\n",
    "        A = A.view(-1, 2, 3)\n",
    "        B = B.view(-1, 2, 3)\n",
    "\n",
    "        return A, B\n",
    "\n",
    "    def cheirality_loss(self, img_pair_shape, pose, grad_dirs, normal_flow, device):\n",
    "        B, _, H, W = img_pair_shape\n",
    "        V, W_ = pose[:, :, :3], pose[:, :, 3:]  # [B, 1, 3]\n",
    "\n",
    "        A, B_mat = self.construct_A_B(H, W, device)  # [H*W, 2, 3]\n",
    "\n",
    "        # Gradient directions\n",
    "        grad_x, grad_y = grad_dirs  # each: [B, H, W]\n",
    "        gx = torch.stack([grad_x, grad_y], dim=-1).view(B, H*W, 2)  # [B, H*W, 2]\n",
    "        gx_unit = F.normalize(gx, dim=-1)  # [B, H*W, 2]\n",
    "\n",
    "        # Normal flow: [B, 2, H, W] -> [B, H*W, 2]\n",
    "        nf = normal_flow.permute(0, 2, 3, 1).reshape(B, H*W, 2)\n",
    "\n",
    "        # Compute scalar projection of normal flow onto gradient direction\n",
    "        nf_scalar = torch.sum(nf * gx_unit, dim=-1)  # [B, H*W]\n",
    "\n",
    "        # gA = gx · A → [B, H*W, 3]\n",
    "        gA = torch.einsum('bpi,pij->bpj', gx_unit, A)\n",
    "\n",
    "        # gB = gx · B → [B, H*W, 3]\n",
    "        gB = torch.einsum('bpi,pij->bpj', gx_unit, B_mat)\n",
    "\n",
    "        # term1 = gA @ V → [B, H*W]\n",
    "        term1 = torch.sum(gA * V, dim=-1)\n",
    "\n",
    "        # term2 = nf_scalar - (gB @ Omega) → [B, H*W]\n",
    "        term2 = nf_scalar - torch.sum(gB * W_, dim=-1)\n",
    "\n",
    "        # rho = term1 * term2\n",
    "        rho = term1 * term2  # [B, H*W]\n",
    "        # Penalize negative values of rho\n",
    "        loss = F.gelu(-rho).mean()\n",
    "        return loss\n",
    "\n",
    "    def refine_pose(self, img_pair_shape, Pec, grad_dirs, normal_flow):\n",
    "        B = Pec.shape[0]\n",
    "        Per = Pec.clone().detach().requires_grad_(True)\n",
    "\n",
    "        optimizer = torch.optim.LBFGS([Per], max_iter=100, line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.cheirality_loss(img_pair_shape, Per, grad_dirs, normal_flow, Pec.device)\n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_([Per], max_norm=100)\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        # Cheirality loss after refinement\n",
    "        loss_cheirality = self.cheirality_loss(img_pair_shape, Per, grad_dirs, normal_flow, Pec.device)\n",
    "\n",
    "        # Compute dL/dPer (gradient of cheirality loss wrt Per)\n",
    "        grad_cheirality = grad(loss_cheirality, Per, create_graph=True)[0]  # shape: [B, 6]\n",
    "\n",
    "        return Per.detach().requires_grad_(), grad_cheirality\n",
    "\n",
    "    def upper_level_loss(self, img_pair_shape, Pec, Per, n_flow_pred, grad_dirs, device):\n",
    "        B, _, H, W = img_pair_shape\n",
    "        A, B_mat = self.construct_A_B(H, W, device)  # [H*W, 2, 3]\n",
    "\n",
    "        grad_x, grad_y = grad_dirs  # each [B, H, W]\n",
    "\n",
    "        # Stack and reshape gradient directions: [B, H, W, 2] -> [B, H*W, 2]\n",
    "        g_x = torch.stack([grad_x, grad_y], dim=-1).view(B, H*W, 2)\n",
    "        gx_unit = F.normalize(g_x, dim=-1)  # normalize gradient directions\n",
    "\n",
    "        # Reshape predicted normal flow vector to [B, H*W, 2]\n",
    "        n_flow_vec = n_flow_pred.permute(0, 2, 3, 1).reshape(B, H*W, 2)  # [B, H*W, 2]\n",
    "\n",
    "        # Project predicted normal flow vector along gradient directions (scalar)\n",
    "        n_flow_scalar = torch.sum(n_flow_vec * gx_unit, dim=-1)  # [B, H*W]\n",
    "\n",
    "        # Pose splits (translation and rotation)\n",
    "        V_r, Omega_r = Per[:,:, :3], Per[:,:, 3:]  # [B, 3]\n",
    "        V_c, Omega_c = Pec[:,:, :3], Pec[:,:, 3:]  # [B, 3]\n",
    "\n",
    "        # Project A and B matrices along gradient directions\n",
    "        g_dot_A = torch.einsum(\"bpi,pij->bpj\", gx_unit, A)      # [B, H*W, 3]\n",
    "        g_dot_B = torch.einsum(\"bpi,pij->bpj\", gx_unit, B_mat)  # [B, H*W, 3]\n",
    "\n",
    "        # Compute derotation term: (gx · B) · Omega_r\n",
    "        g_dot_B_Omega_r = torch.sum(g_dot_B * Omega_r.unsqueeze(1), dim=-1)  # [B, H*W]\n",
    "\n",
    "        # Compute translational term: (gx · A) · V_r\n",
    "        g_dot_A_V_r = torch.sum(g_dot_A * V_r.unsqueeze(1), dim=-1)  # [B, H*W]\n",
    "\n",
    "        # Compute denominator of depth scale: n_x - (gx · B) · Omega_r\n",
    "        denom = n_flow_scalar - g_dot_B_Omega_r  # [B, H*W]\n",
    "        eps = 1e-6\n",
    "        depth_scale = g_dot_A_V_r / (denom + eps)  # [B, H*W]\n",
    "\n",
    "        # Compute terms for coarse pose Pec\n",
    "        g_dot_A_V_c = torch.sum(g_dot_A * V_c.unsqueeze(1), dim=-1)  # [B, H*W]\n",
    "        g_dot_B_Omega_c = torch.sum(g_dot_B * Omega_c.unsqueeze(1), dim=-1)  # [B, H*W]\n",
    "\n",
    "        # Predict normal flow scalar from coarse pose Pec using depth scale\n",
    "        n_flow_pred_from_pose = (g_dot_A_V_c / (depth_scale + eps)) - g_dot_B_Omega_c  # [B, H*W]\n",
    "\n",
    "        # Compute MSE loss between predicted scalar normal flow and model predicted scalar normal flow\n",
    "        n_flow_pred_from_pose = n_flow_pred_from_pose.squeeze(1)  # now shape [1, 307200]\n",
    "\n",
    "        loss = F.mse_loss(n_flow_pred_from_pose, n_flow_scalar)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def forward(self, img_pair):\n",
    "        img1, img2,img3,img4,img5,img6 = torch.chunk(img_pair, chunks=6, dim=1)\n",
    "\n",
    "        # Remove the singleton dimension (1) at dim=1\n",
    "        img1 = img1.squeeze(1)  # shape: [1, 3, 480, 640]\n",
    "        img2 = img2.squeeze(1)\n",
    "        img3 = img3.squeeze(1)\n",
    "        img4 = img4.squeeze(1)\n",
    "        img5 = img5.squeeze(1)\n",
    "        img6 = img6.squeeze(1)  # [1, 3, 480, 640]\n",
    "        # Resize & normalize manually\n",
    "        def preprocess(img):\n",
    "            img = F.interpolate(img, size=(224, 224), mode='bilinear', align_corners=False)  # [B, 3, 224, 224]\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406], device=img.device).view(1, 3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225], device=img.device).view(1, 3, 1, 1)\n",
    "            img = (img - mean) / std\n",
    "            return img\n",
    "\n",
    "        a_ = preprocess(img1)  # [B, 3, 224, 224]\n",
    "        b_ = preprocess(img2)\n",
    "        c_ = preprocess(img3)  # [B, 3, 224, 224]\n",
    "        d_ = preprocess(img4)\n",
    "        e_ = preprocess(img5)  # [B, 3, 224, 224]\n",
    "        f_ = preprocess(img6)  # [B, 3, 224, 224]\n",
    "\n",
    "        pose_img = torch.stack([a_, b_,c_,d_,e_,f_], dim=1) \n",
    "    \n",
    "        translation, rotation = self.posenet(pose_img)\n",
    "        print(\"Posenet Çıktıları:\")\n",
    "        print(\"Posenet Çıktıları Translation:\")\n",
    "\n",
    "        translation = translation[:,:1,:]\n",
    "        print(translation)\n",
    "        rotation = rotation[:,:1,:]\n",
    "        print(\"Posenet Çıktıları Quaternion:\")\n",
    "        print(rotation)\n",
    "        q_flat = rotation.reshape(-1, 4)\n",
    "        # Convert to rotation vector (axis-angle): [B * T, 3]\n",
    "        rotvec_flat = kornia.geometry.quaternion_to_axis_angle(q_flat)\n",
    "        print(\"Posenet Çıktıları Rotation:\")\n",
    "        print(rotvec_flat)\n",
    "        # reshape: [B, T, 3]\n",
    "        rotation = rotvec_flat.view(rotation.shape[0], rotation.shape[1], 3)\n",
    "        Pec = torch.cat([translation, rotation], dim=-1) \n",
    "        img_pair = img_pair[:,:2]\n",
    "        img_pair = img_pair.view(1, 2 * 3, 480, 640)\n",
    "        nflow_pred = self.nflownet(img_pair)\n",
    "        nflow_pred = nflow_pred[:, :, 1:-1, 1:-1]  # [1, 2, 478, 638]\n",
    "        gray = nflow_pred[:, :3].mean(1, keepdim=True)\n",
    "        grad_dirs = compute_image_gradients(gray)  # [B, 2, H, W]\n",
    "        img_rand = torch.rand(1, 6, 478, 638)\n",
    "        img_pair_shape = img_rand.shape\n",
    "        # Lower-level: refine using cheirality\n",
    "        Per, dL_dPer = self.refine_pose(img_pair_shape, Pec, grad_dirs, nflow_pred)       \n",
    "        print(\"Refined Pose:\")\n",
    "        print(Per)\n",
    "        upper_loss = self.upper_level_loss(img_pair_shape, Pec, Per, nflow_pred, grad_dirs,img_pair.device)\n",
    "        \n",
    "        # ∂L_upper / ∂Pec + ∂L_upper / ∂Per * ∂Per / ∂Pec\n",
    "        dL_dPec_upper = grad(upper_loss, Pec, retain_graph=True, create_graph=True)[0]\n",
    "        dL_dPer_upper = grad(upper_loss, Per, retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        # ∂L_total / ∂Pec = ∂L_upper / ∂Pec - dL_dPer_upper ⊙ d_cheirality_loss / d_Per\n",
    "        total_grad = dL_dPec_upper - torch.autograd.grad((dL_dPer * dL_dPer_upper).sum(), Pec, retain_graph=True)[0]\n",
    "\n",
    "        # Kayıp gibi davranarak backward yap\n",
    "        dummy_loss = (Pec * total_grad.detach()).sum()\n",
    "        return dummy_loss\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheirality_layer = CheiralityLayer(nflownet=nflownet,posenet=posenet).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Dataset and DataLoader\n",
    "dataset = TartanAirDataset(root_dir=\"D:/KOC UNIVERSITY/COMP447/tartanair_dataset\",seq_len=6)\n",
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Translation:\n",
      "tensor([[[ 0.0387, -0.2009,  0.0501]]])\n",
      "Ground Truth Quaternion:\n",
      "tensor([[[ 6.7457e-09, -4.7890e-08, -3.5645e-03,  9.9999e-01]]])\n",
      "Ground Truth Rotation:\n",
      "tensor([[-1.5045e-07, -1.1198e-02,  3.1416e+00]])\n",
      "Posenet Çıktıları:\n",
      "Posenet Çıktıları Translation:\n",
      "tensor([[[-0.0020,  0.0042, -0.0034]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Posenet Çıktıları Quaternion:\n",
      "tensor([[[-0.4018, -0.3338, -0.0230, -0.8524]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Posenet Çıktıları Rotation:\n",
      "tensor([[0.8438, 0.0581, 2.1545]], device='cuda:0',\n",
      "       grad_fn=<AsStridedBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(posenet.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for images, translation, rotation in train_loader:\n",
    "        print(\"Ground Truth Translation:\")\n",
    "        print(translation[:,:1,:])\n",
    "        print(\"Ground Truth Quaternion:\")\n",
    "        rotation = rotation[:,:1,:]\n",
    "        print(rotation)\n",
    "        q_flat = rotation.reshape(-1, 4)\n",
    "        # Convert to rotation vector (axis-angle): [B * T, 3]\n",
    "        rotvec_flat = kornia.geometry.quaternion_to_axis_angle(q_flat)\n",
    "        print(\"Ground Truth Rotation:\")\n",
    "        print(rotvec_flat)\n",
    "        images = images.to(device)\n",
    "        loss = cheirality_layer(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print(loss.item())\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
