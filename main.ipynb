{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from posenet.model import PoseNetDinoImproved\n",
    "from nflownet.model import NFlowNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from dataset.tartanair import TartanAirDataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_posenet(posenet_path):\n",
    "    # Initialize FlowNet model\n",
    "    posenet = PoseNetDinoImproved().to(device)\n",
    "    \n",
    "    # Load weights from .pth file\n",
    "    checkpoint = torch.load(posenet_path, map_location=device)\n",
    "    \n",
    "    # Handle different save formats\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    elif 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Load state dict into model\n",
    "    posenet.load_state_dict(state_dict)\n",
    "    posenet.train()  # Set to evaluation mode\n",
    "    \n",
    "    return posenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nflownet(nflownet_path):\n",
    "    # Initialize FlowNet model\n",
    "    nflownet = NFlowNet().to(device)\n",
    "    \n",
    "    # Load weights from .pth file\n",
    "    checkpoint = torch.load(nflownet_path, map_location=device)\n",
    "    \n",
    "    # Handle different save formats\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    elif 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Load state dict into model\n",
    "    nflownet.load_state_dict(state_dict)\n",
    "    nflownet.eval()  # Set to evaluation mode\n",
    "    \n",
    "    return nflownet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\emircan/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\emircan/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\emircan/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\emircan/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "C:\\Users\\emircan\\AppData\\Local\\Temp\\ipykernel_5240\\4256205414.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(posenet_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "posenet_path = r\"D:\\KOC UNIVERSITY\\COMP447\\trainedmodels\\posenet\\posenet2.pth\"\n",
    "nflownet_path = r\"D:\\KOC UNIVERSITY\\COMP447\\trainedmodels\\nflownet\\nflownet_final.pth\"\n",
    "\n",
    "posenet = load_posenet(posenet_path)\n",
    "nflownet = load_nflownet(nflownet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from nflownet.utils import compute_image_gradients\n",
    "import kornia\n",
    "from torchvision import transforms\n",
    "\n",
    "class CheiralityLayer(nn.Module):\n",
    "    def __init__(self, posenet, nflownet):\n",
    "        super().__init__()\n",
    "        self.posenet = posenet\n",
    "        self.nflownet = nflownet\n",
    "        self.nflownet.eval()  # freeze NFlowNet\n",
    "        for p in self.nflownet.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "\n",
    "    def construct_A_B(self, H, W, device):\n",
    "        y, x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "\n",
    "        # A and B: [H, W, 2, 3]\n",
    "        A = torch.stack([\n",
    "            -torch.ones_like(x), torch.zeros_like(x), x,\n",
    "            torch.zeros_like(y), -torch.ones_like(y), y\n",
    "        ], dim=-1).reshape(H, W, 2, 3)\n",
    "\n",
    "        B = torch.stack([\n",
    "            x * y, -(x ** 2 + 1), y,\n",
    "            y ** 2 + 1, -x * y, -x\n",
    "        ], dim=-1).reshape(H, W, 2, 3)\n",
    "\n",
    "        # Reshape to [H*W, 2, 3]\n",
    "        A = A.view(-1, 2, 3)\n",
    "        B = B.view(-1, 2, 3)\n",
    "\n",
    "        return A, B\n",
    "\n",
    "    def cheirality_loss(self, img_pair_shape, pose, grad_dirs, normal_flow, device):\n",
    "        B, _, H, W = img_pair_shape\n",
    "        V, W_ = pose[:, :, :3], pose[:, :, 3:]  # [B, 1, 3]\n",
    "\n",
    "        A, B_mat = self.construct_A_B(H, W, device)  # [H*W, 2, 3]\n",
    "\n",
    "        # Gradient directions\n",
    "        grad_x, grad_y = grad_dirs[:, 0, :, :], grad_dirs[:, 1, :, :]\n",
    "        gx = torch.stack([grad_x, grad_y], dim=-1).view(B, H*W, 2)  # [B, H*W, 2]\n",
    "        gx_unit = F.normalize(gx, dim=-1)  # [B, H*W, 2]\n",
    "\n",
    "        # Normal flow: [B, 2, H, W] -> [B, H*W, 2]\n",
    "        nf = normal_flow.permute(0, 2, 3, 1).reshape(B, H*W, 2)\n",
    "\n",
    "        n_flow_scalar = torch.norm(nf, dim=-1)  # [B, H*W]\n",
    "\n",
    "        # gA = gx · A → [B, H*W, 3]\n",
    "        gA = torch.einsum('bpi,pij->bpj', gx_unit, A)\n",
    "\n",
    "        # gB = gx · B → [B, H*W, 3]\n",
    "        gB = torch.einsum('bpi,pij->bpj', gx_unit, B_mat)\n",
    "\n",
    "        # term1 = gA @ V → [B, H*W]\n",
    "        term1 = torch.sum(gA * V, dim=-1)\n",
    "\n",
    "        # term2 = nf_scalar - (gB @ Omega) → [B, H*W]\n",
    "        term2 = n_flow_scalar - torch.sum(gB * W_, dim=-1)\n",
    "\n",
    "        # rho = term1 * term2\n",
    "        rho = term1 * term2  # [B, H*W]\n",
    "        # Penalize negative values of rho\n",
    "        loss = F.gelu(-rho).mean()\n",
    "        return loss\n",
    "\n",
    "    def refine_pose(self, img_pair_shape, Pec, grad_dirs, normal_flow):\n",
    "        B = Pec.shape[0]\n",
    "        Per = Pec.clone().detach().requires_grad_(True)\n",
    "\n",
    "        optimizer = torch.optim.LBFGS([Per], max_iter=100, line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.cheirality_loss(img_pair_shape, Per, grad_dirs, normal_flow, Pec.device)\n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_([Per], max_norm=100)\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        # Cheirality loss after refinement\n",
    "        loss_cheirality = self.cheirality_loss(img_pair_shape, Per, grad_dirs, normal_flow, Pec.device)\n",
    "\n",
    "        # Compute dL/dPer (gradient of cheirality loss wrt Per)\n",
    "        grad_cheirality = grad(loss_cheirality, Per, create_graph=True)[0]  # shape: [B, 6]\n",
    "\n",
    "        return Per.detach().requires_grad_(), grad_cheirality\n",
    "\n",
    "    def upper_level_loss(self, img_pair_shape, Pec, Per, n_flow_pred, grad_dirs, device):\n",
    "        B, _, H, W = img_pair_shape\n",
    "        A, B_mat = self.construct_A_B(H, W, device)  # [H*W, 2, 3]\n",
    "\n",
    "        # Gradients x ve y'yi ayır ve [B, H*W, 2] şekline getir\n",
    "        grad_x, grad_y = grad_dirs[:, 0, :, :], grad_dirs[:, 1, :, :]\n",
    "        gx = torch.stack([grad_x, grad_y], dim=-1).view(B, H * W, 2)  # [B, H*W, 2]\n",
    "        gx_unit = F.normalize(gx, dim=-1)  # Birim vektörler [B, H*W, 2]\n",
    "\n",
    "        # Normal flow tahminini [B, H*W, 2] şekline getir\n",
    "        nf = n_flow_pred.permute(0, 2, 3, 1).reshape(B, H * W, 2)  # [B, H*W, 2]\n",
    "\n",
    "        # Pose vektörlerini çevirme (translation) ve dönme (rotation) olarak ayır\n",
    "        V_r, Omega_r = Per[:, :, :3], Per[:, :, 3:]  # [B, 3]\n",
    "        V_c, Omega_c = Pec[:, :, :3], Pec[:, :, 3:]  # [B, 3]\n",
    "\n",
    "        # A ve B matrislerini gradyan yönlerine projekte et\n",
    "        gA = torch.einsum('bpi,pij->bpj', gx_unit, A)      # [B, H*W, 3]\n",
    "        gB = torch.einsum('bpi,pij->bpj', gx_unit, B_mat)  # [B, H*W, 3]\n",
    "\n",
    "        # Derinlik ölçeğini Per'den hesapla\n",
    "        g_dot_B_Omega_r = torch.sum(gB * Omega_r.unsqueeze(1), dim=-1)  # [B, H*W]\n",
    "        g_dot_A_V_r = torch.sum(gA * V_r.unsqueeze(1), dim=-1)          # [B, H*W]\n",
    "        n_flow_scalar = torch.norm(nf, dim=-1)                          # [B, H*W]\n",
    "\n",
    "        denom = n_flow_scalar - g_dot_B_Omega_r  # Stabilite için payda\n",
    "        depth_scale = g_dot_A_V_r / (denom + 10e-6)        # [B, H*W]\n",
    "\n",
    "        # Pec'den benzer hesaplamalar\n",
    "        g_dot_A_V_c = torch.sum(gA * V_c.unsqueeze(1), dim=-1)\n",
    "        g_dot_B_Omega_c = torch.sum(gB * Omega_c.unsqueeze(1), dim=-1)\n",
    "\n",
    "        # Derinlik ile normal flow tahmini yeniden oluşturuluyor\n",
    "        flow_from_pose = g_dot_A_V_c / depth_scale  # [B, H*W, 2]\n",
    "        flow_from_pose = flow_from_pose + g_dot_B_Omega_c         # [B, H*W, 2]\n",
    "\n",
    "        print(flow_from_pose.shape)\n",
    "\n",
    "\n",
    "        # Tahmin edilen normal flow ile rekonstrüksiyon arasındaki MSE kaybını hesapla\n",
    "        loss = F.mse_loss(flow_from_pose, n_flow_scalar)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, img_pair):\n",
    "        img1, img2,img3,img4,img5,img6 = torch.chunk(img_pair, chunks=6, dim=1)\n",
    "\n",
    "        # Remove the singleton dimension (1) at dim=1\n",
    "        img1 = img1.squeeze(1)  # shape: [1, 3, 480, 640]\n",
    "        img2 = img2.squeeze(1)\n",
    "        img3 = img3.squeeze(1)\n",
    "        img4 = img4.squeeze(1)\n",
    "        img5 = img5.squeeze(1)\n",
    "        img6 = img6.squeeze(1)  # [1, 3, 480, 640]\n",
    "        # Resize & normalize manually\n",
    "        def preprocess(img):\n",
    "            img = F.interpolate(img, size=(224, 224), mode='bilinear', align_corners=False)  # [B, 3, 224, 224]\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406], device=img.device).view(1, 3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225], device=img.device).view(1, 3, 1, 1)\n",
    "            img = (img - mean) / std\n",
    "            return img\n",
    "\n",
    "        a_ = preprocess(img1)  # [B, 3, 224, 224]\n",
    "        b_ = preprocess(img2)\n",
    "        c_ = preprocess(img3)  # [B, 3, 224, 224]\n",
    "        d_ = preprocess(img4)\n",
    "        e_ = preprocess(img5)  # [B, 3, 224, 224]\n",
    "        f_ = preprocess(img6)  # [B, 3, 224, 224]\n",
    "\n",
    "        pose_img = torch.stack([a_, b_,c_,d_,e_,f_], dim=1) \n",
    "\n",
    "        # Ground truth translation (shape: [1, 1, 3])\n",
    "        eps = 10e-6\n",
    "        gt_translation = torch.tensor([[[ 0.0387, -0.2009,  0.0501]]], dtype=torch.float32, device=img_pair.device, requires_grad=True)\n",
    "\n",
    "        # Ground truth rotation (shape: [1, 3])\n",
    "        gt_rotation = torch.tensor([[-1.5045e-07, -1.1198e-02, 3.1416e+00]], dtype=torch.float32, device=img_pair.device, requires_grad=True)\n",
    "\n",
    "        gt_rotation = gt_rotation.unsqueeze(1) \n",
    "    \n",
    "        translation, rotation = self.posenet(pose_img)\n",
    "        print(\"Posenet Çıktıları:\")\n",
    "        print(\"Posenet Çıktıları Translation:\")\n",
    "\n",
    "        #translation = translation[:,:1,:]\n",
    "        translation = gt_translation\n",
    "        print(translation)\n",
    "        #rotation = rotation[:,:1,:]\n",
    "        q_flat = rotation.reshape(-1, 4)\n",
    "        # Convert to rotation vector (axis-angle): [B * T, 3]\n",
    "        rotvec_flat = kornia.geometry.quaternion_to_axis_angle(q_flat)\n",
    "        # reshape: [B, T, 3]\n",
    "        rotation = rotvec_flat.view(rotation.shape[0], rotation.shape[1], 3)\n",
    "        rotation = gt_rotation\n",
    "        print(\"Posenet Çıktıları Rotation:\")\n",
    "        print(rotation)\n",
    "        Pec = torch.cat([translation, rotation], dim=-1) \n",
    "        img_pair = img_pair[:,:2]\n",
    "        img_pair = img_pair.view(1, 2 * 3, 480, 640)\n",
    "        nflow_pred = self.nflownet(img_pair)\n",
    "        nflow_pred = nflow_pred[:, :, 1:-1, 1:-1]  # [1, 2, 478, 638]\n",
    "        gray = nflow_pred[:, :3].mean(1, keepdim=True)\n",
    "        grad_dirs = compute_image_gradients(gray)  # [B, 2, H, W]\n",
    "\n",
    "        print(grad_dirs) \n",
    "\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device=img1.device).view(1, 1, 3, 3)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32, device=img1.device).view(1, 1, 3, 3)\n",
    "        \n",
    "        # Compute gradients for both images and average\n",
    "        grad_x1 = F.conv2d(nflow_pred[:, 0:1, :, :], sobel_x, padding=1)\n",
    "        grad_y1 = F.conv2d(nflow_pred[:, 0:1, :, :], sobel_y, padding=1)\n",
    "        grad_x2 = F.conv2d(nflow_pred[:, 1:2, :, :], sobel_x, padding=1)\n",
    "        grad_y2 = F.conv2d(nflow_pred[:, 1:2, :, :], sobel_y, padding=1)\n",
    "\n",
    "        image_gradients = 0.5 * (torch.cat([grad_x1, grad_y1], dim=1) + torch.cat([grad_x2, grad_y2], dim=1))  # [B, 2, H, W]\n",
    "        print(image_gradients.shape)\n",
    "\n",
    "        img_rand = torch.rand(1, 6, 478, 638)\n",
    "        img_pair_shape = img_rand.shape\n",
    "        # Lower-level: refine using cheirality\n",
    "        Per, dL_dPer = self.refine_pose(img_pair_shape, Pec, image_gradients, nflow_pred)       \n",
    "        print(\"Refined Pose:\")\n",
    "        Per = Pec\n",
    "        print(Per)\n",
    "        upper_loss = self.upper_level_loss(img_pair_shape, Pec, Per, nflow_pred, image_gradients,img_pair.device)\n",
    "        print(\"Upper level loss:\")\n",
    "        print(upper_loss)\n",
    "        # ∂L_upper / ∂Pec + ∂L_upper / ∂Per * ∂Per / ∂Pec\n",
    "        dL_dPec_upper = grad(upper_loss, Pec, retain_graph=True, create_graph=True)[0]\n",
    "        dL_dPer_upper = grad(upper_loss, Per, retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        # ∂L_total / ∂Pec = ∂L_upper / ∂Pec - dL_dPer_upper ⊙ d_cheirality_loss / d_Per\n",
    "        total_grad = dL_dPec_upper - torch.autograd.grad((dL_dPer * dL_dPer_upper).sum(), Pec, retain_graph=True)[0]\n",
    "\n",
    "        # Kayıp gibi davranarak backward yap\n",
    "        dummy_loss = (Pec * total_grad.detach()).sum()\n",
    "        return dummy_loss\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheirality_layer = CheiralityLayer(nflownet=nflownet,posenet=posenet).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "dataset = TartanAirDataset(root_dir=\"D:/KOC UNIVERSITY/COMP447/tartanair_dataset\",seq_len=6)\n",
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Translation:\n",
      "tensor([[[ 0.0387, -0.2009,  0.0501]]])\n",
      "Ground Truth Rotation:\n",
      "tensor([[-1.5045e-07, -1.1198e-02,  3.1416e+00]])\n",
      "Posenet Çıktıları:\n",
      "Posenet Çıktıları Translation:\n",
      "tensor([[[ 0.0387, -0.2009,  0.0501]]], device='cuda:0', requires_grad=True)\n",
      "Posenet Çıktıları Rotation:\n",
      "tensor([[[-1.5045e-07, -1.1198e-02,  3.1416e+00]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "(tensor([[[-2.6092e-03,  3.3763e-02,  5.2077e-03,  ...,  3.3663e+00,\n",
      "           8.8877e-01, -2.6846e+00],\n",
      "         [ 2.5211e-02, -5.0654e-03, -1.9417e-02,  ...,  4.1607e+00,\n",
      "           4.5573e-01, -3.2379e+00],\n",
      "         [ 5.3067e-02, -1.8799e-01, -2.0356e-02,  ...,  1.6967e+00,\n",
      "          -2.2123e+00, -9.9573e-01],\n",
      "         ...,\n",
      "         [-3.1697e-02, -4.3806e-03, -5.2419e-01,  ..., -3.7698e-01,\n",
      "           6.5062e-01,  1.7330e-01],\n",
      "         [ 7.8684e-02,  1.1772e-01, -4.9626e-01,  ...,  3.3387e-01,\n",
      "           7.6701e-01, -4.3444e-01],\n",
      "         [ 4.2205e-02, -2.8262e-02, -2.5247e-01,  ...,  4.2837e-01,\n",
      "           5.3135e-01, -4.1007e-01]]], device='cuda:0'), tensor([[[-2.6092e-03,  3.0589e-02,  6.9560e-02,  ...,  1.0249e+00,\n",
      "           2.5615e+00,  1.4037e+00],\n",
      "         [ 2.8385e-02, -1.5821e-02, -1.1824e-01,  ...,  8.7414e-02,\n",
      "          -1.0878e+00, -9.1636e-01],\n",
      "         [-5.2918e-04, -1.1139e-01, -1.9283e-01,  ...,  2.7017e+00,\n",
      "          -1.2551e+00, -1.8524e+00],\n",
      "         ...,\n",
      "         [ 1.0832e-01,  4.6184e-01,  6.2769e-01,  ...,  1.3481e+00,\n",
      "           1.5907e+00,  1.2250e+00],\n",
      "         [ 1.3013e-02, -1.0802e-01, -1.2384e-01,  ...,  2.8429e-02,\n",
      "           6.1299e-01,  4.8744e-01],\n",
      "         [-4.6022e-02, -1.0745e-01,  6.1691e-03,  ..., -7.7998e-01,\n",
      "          -1.5057e+00, -1.5915e+00]]], device='cuda:0'))\n",
      "torch.Size([1, 2, 478, 638])\n",
      "Refined Pose:\n",
      "tensor([[[ 3.8700e-02, -2.0090e-01,  5.0100e-02, -1.5045e-07, -1.1198e-02,\n",
      "           3.1416e+00]]], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "torch.Size([1, 1, 304964])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 478, 638, 2]' is invalid for input of size 304964",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(rotvec_flat)\n\u001b[0;32m     16\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcheirality_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 226\u001b[0m, in \u001b[0;36mCheiralityLayer.forward\u001b[1;34m(self, img_pair)\u001b[0m\n\u001b[0;32m    224\u001b[0m Per \u001b[38;5;241m=\u001b[39m Pec\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28mprint\u001b[39m(Per)\n\u001b[1;32m--> 226\u001b[0m upper_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper_level_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_pair_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnflow_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_gradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg_pair\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper level loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28mprint\u001b[39m(upper_loss)\n",
      "Cell \u001b[1;32mIn[5], line 136\u001b[0m, in \u001b[0;36mCheiralityLayer.upper_level_loss\u001b[1;34m(self, img_pair_shape, Pec, Per, n_flow_pred, grad_dirs, device)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(flow_from_pose\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Görüntü formatına geri çevir\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m flow_from_pose_2d \u001b[38;5;241m=\u001b[39m \u001b[43mflow_from_pose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [B, 2, H, W]\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Tahmin edilen normal flow ile rekonstrüksiyon arasındaki MSE kaybını hesapla\u001b[39;00m\n\u001b[0;32m    139\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(flow_from_pose_2d, n_flow_pred)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 478, 638, 2]' is invalid for input of size 304964"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(posenet.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for images, translation, rotation in train_loader:\n",
    "        print(\"Ground Truth Translation:\")\n",
    "        print(translation[:,:1,:])\n",
    "        rotation = rotation[:,:1,:]\n",
    "        q_flat = rotation.reshape(-1, 4)\n",
    "        # Convert to rotation vector (axis-angle): [B * T, 3]\n",
    "        rotvec_flat = kornia.geometry.quaternion_to_axis_angle(q_flat)\n",
    "        print(\"Ground Truth Rotation:\")\n",
    "        print(rotvec_flat)\n",
    "        images = images.to(device)\n",
    "        loss = cheirality_layer(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print(loss.item())\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
